{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "Find items that do not follow the distribution of the majority of data\n",
    "![](images/anomalies.png)\n",
    "*Taken from https://arxiv.org/pdf/1901.03407.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of models\n",
    "- Semi-supervised\n",
    "- Unsupervised\n",
    "- Hybrid\n",
    "- One-Class Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoder\n",
    "\n",
    "\n",
    "![](images/autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "Time Series Anomaly Detection\n",
    "\n",
    "![](images/time_series.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "Feature Extraction / Data Compression\n",
    "\n",
    "![](images/feature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "Feature Extraction / Data Compression\n",
    "\n",
    "![](images/denoising.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types\n",
    "- Vanilla autoencoder\n",
    "- Deep autoencoder\n",
    "- Convolutional autoencoder\n",
    "- Spati-temporal autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vanilla Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "class Autoencoder(gluon.Block):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.encoder = gluon.nn.Dense(128, activation='relu')\n",
    "            self.decoder = gluon.nn.Dense(28 * 28, activation='tanh')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoencoder(gluon.Block):\n",
    "    def __init__(self):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.encoder = gluon.nn.Sequential()\n",
    "            with self.encoder.name_scope():\n",
    "                self.encoder.add(gluon.nn.Dense(128, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.Dense(64, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.Dense(12, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.Dense(3))\n",
    "\n",
    "            self.decoder = gluon.nn.Sequential()\n",
    "            with self.decoder.name_scope():\n",
    "                self.decoder.add(gluon.nn.Dense(12, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.Dense(64, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.Dense(128, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.Dense(28 * 28, activation='tanh'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Convolutional Autoenocoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(gluon.nn.HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.encoder = gluon.nn.HybridSequential(prefix=\"\")\n",
    "            with self.encoder.name_scope():\n",
    "                self.encoder.add(gluon.nn.Conv2D(32, 5, padding=0, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.MaxPool2D(2))\n",
    "                self.encoder.add(gluon.nn.Conv2D(32, 5, padding=0, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.MaxPool2D(2))\n",
    "                self.encoder.add(gluon.nn.Dense(2000))\n",
    "            self.decoder = gluon.nn.HybridSequential(prefix=\"\")\n",
    "            with self.decoder.name_scope():\n",
    "                self.decoder.add(gluon.nn.Dense(32*22*22, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.HybridLambda(lambda F, x: F.UpSampling(x, scale=2, sample_type='nearest')))\n",
    "                self.decoder.add(gluon.nn.Conv2DTranspose(32, 5, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.HybridLambda(lambda F, x: F.UpSampling(x, scale=2, sample_type='nearest')))\n",
    "                self.decoder.add(gluon.nn.Conv2DTranspose(1, kernel_size=5, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder[0](x)\n",
    "        # need to reshape ouput feature vector from Dense(32*22*22), before it is upsampled\n",
    "        x = x.reshape((-1,32,22,22))\n",
    "        x = self.decoder[1:](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Anomaly Detection with Convolutional Autoencoders\n",
    "\n",
    "UCSD dataset:\n",
    "![](images/ucsd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "![](images/autoencoder_dense.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "Without bottlneck layer, the model can not learn meaningful features:\n",
    "![](images/autoencoder_nodense.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Takeaways\n",
    "\n",
    "No size fits all:\n",
    "\n",
    "- how much variation in the training set\n",
    "- size of anomalies\n",
    "- type of data (image, audio, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomaly Detection in Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem of the previous model:\n",
    "- we do not assume any dependency/order between images\n",
    "- need to train model on sequence of images\n",
    "- need to use LSTM cells to learn sequence\n",
    "\n",
    "![](images/Picture9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spatio-temporal Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMAE(gluon.nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ConvLSTMAE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "\n",
    "          self.encoder = gluon.nn.HybridSequential()\n",
    "          self.encoder.add(gluon.nn.Conv2D(128, kernel_size=11, strides=4, activation='relu'))\n",
    "          self.encoder.add(gluon.nn.Conv2D(64, kernel_size=5, strides=2, activation='relu'))\n",
    "\n",
    "          self.temporal_encoder = gluon.rnn.HybridSequentialRNNCell()\n",
    "          self.temporal_encoder.add(gluon.contrib.rnn.Conv2DLSTMCell((64,26,26), 64, 3, 3, i2h_pad=1))\n",
    "          self.temporal_encoder.add(gluon.contrib.rnn.Conv2DLSTMCell((64,26,26), 32, 3, 3, i2h_pad=1))\n",
    "          self.temporal_encoder.add(gluon.contrib.rnn.Conv2DLSTMCell((32,26,26), 64, 3, 3, i2h_pad=1))\n",
    "\n",
    "          self.decoder =  gluon.nn.HybridSequential()\n",
    "          self.decoder.add(gluon.nn.Conv2DTranspose(channels=128, kernel_size=5, strides=2, activation='relu'))\n",
    "          self.decoder.add(gluon.nn.Conv2DTranspose(channels=10, kernel_size=11, strides=4, activation='sigmoid'))\n",
    "\n",
    "    def hybrid_forward(self, F, x, states=None, **kwargs):\n",
    "        x = self.encoder(x)\n",
    "        x, states = self.temporal_encoder(x, states)\n",
    "        x = self.decoder(x)\n",
    "        return x, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional LSTM\n",
    "*Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting* (NIPS 2015)\n",
    "\n",
    "#### Fully Connected LSTM:\n",
    "- powerful for handling temporal data\n",
    "- too much redundancy for spatial data\n",
    "- input-to-state and state-to-state transitions do not encode spatial information\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "*Taken from https://colah.github.io/posts/2015-08-Understanding-LSTMs/* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Convolutional LSTM:\n",
    "- determines future state of a cell based on inputs and past states of its neighbors\n",
    "![](https://cdn-images-1.medium.com/max/640/1*u8neecA4w6b_F1NgnyPP0Q.png)\n",
    "*Taken from https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab: find anamlous movements and objects in UCSD dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import modules and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "import tarfile\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ctx = mx.cpu()\n",
    "num_epochs = 80\n",
    "batch_size = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Download dataset \n",
    "if not os.path.isfile(\"UCSD_Anomaly_Dataset.tar.gz\"):\n",
    "  response = request.urlretrieve(\"http://www.svcl.ucsd.edu/projects/anomaly/UCSD_Anomaly_Dataset.tar.gz\", \"UCSD_Anomaly_Dataset.tar.gz\")\n",
    "  tar = tarfile.open(\"UCSD_Anomaly_Dataset.tar.gz\")\n",
    "  tar.extractall()\n",
    "  tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequence of 10 images\n",
    "def create_dataset_stacked_images(path, batch_size, shuffle):\n",
    "  \n",
    "    files = sorted(glob.glob(path))\n",
    "    n = int(len(files)/10)\n",
    "    data = np.zeros((n,10,227,227))\n",
    "    for idx in range(n):\n",
    "        for i in range(0,10):\n",
    "            im = Image.open(files[(idx*10)+i])\n",
    "            im = im.resize((227,227))\n",
    "            data[idx,i,:,:] = np.array(im, dtype=np.float32)/255.0\n",
    "            \n",
    "    dataset = gluon.data.ArrayDataset(mx.nd.array(data, dtype=np.float32))\n",
    "    dataloader = gluon.data.DataLoader(dataset, batch_size=batch_size, last_batch=\"rollover\", shuffle=shuffle)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Train data\n",
    "train_dataloader = create_dataset_stacked_images(\"UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/*/*\", shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Test data\n",
    "test_dataloader = create_dataset_stacked_images(\"UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test/*/*\", shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "model = ConvLSTMAE()\n",
    "model.hybridize()\n",
    "#model.collect_params().initialize(mx.init.Xavier(), ctx=mx.gpu())\n",
    "model.load_parameters(\"convLSTMAE.params\", ctx=ctx)\n",
    "states = model.temporal_encoder.begin_state(batch_size=batch_size, ctx=ctx)\n",
    "\n",
    "# Loss\n",
    "l2loss = gluon.loss.L2Loss()\n",
    "\n",
    "# Trainer\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 1e-4, 'wd': 1e-5, 'epsilon':1e-6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/80], loss:0.0017\n"
     ]
    }
   ],
   "source": [
    " # Start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for image in train_dataloader:\n",
    "\n",
    "        image  = image.as_in_context(ctx)\n",
    "        states = model.temporal_encoder.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=ctx)\n",
    "\n",
    "        with mx.autograd.record():\n",
    "            reconstructed, states = model(image, states)\n",
    "            loss = l2loss(reconstructed, image)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step(batch_size)\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, mx.nd.mean(loss).asscalar()))\n",
    "\n",
    "#Save parameters\n",
    "model.save_parameters(\"mymodel.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Inference\n",
    "![](images/lstmae.gif)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
